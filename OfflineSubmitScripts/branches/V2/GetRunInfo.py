#!/usr/bin/env python

"""
Gather information about recent runs from live database and migrate it to
the prodcution databases (i3filter on filter-db).
"""

from libs.config import get_config
from libs.argparser import get_defaultparser
from libs.logger import get_logger
from libs.path import get_logdir
from libs.runs import Run, validate_file_integrity
from libs.databaseconnection import DatabaseConnection

import os

def send_check_notification(new_records, changed_records, logger, dryrun):
    from libs.email import send_email

    receivers = get_config(logger).get_var_list('GetRunInfo', 'NotificationReceiver')

    message = """
        *** This is an automated message generated by the *** <br>
        ***        GetRunInfo system of L2 Processing     *** <br><br>
        
        A new snapshot is available!

        New Runs: {new_runs}
        Changed Runs: {changed_runs}
        """.format(new_runs = new_records, changed_runs = changed_records)

    send_email(receivers, 'A new snapshot is available!', message, logger, dryrun)

def main(inputfiletype, logger, dryrun, check):
    if check:
        logger.info('Only in check mode. Just checking if an update is available.')

    db_filter = DatabaseConnection.get_connection('filter-db', logger)
    db_i3live = DatabaseConnection.get_connection('i3live', logger)

    config = get_config(logger)

    # Get the current production version and snapshot info
    # from the production database
    current_info = db_filter.fetchall("""
        SELECT
            MAX(snapshot_id) as max_snapshot_id,
            MAX(production_version) as max_production_version
        FROM i3filter.runs
    """)[0]

    current_info['snapshot_id'] = current_info['max_snapshot_id'] or 0
    current_info['production_version'] = current_info['max_production_version'] or 0

    logger.debug("Got current max production_version {max_production_version} and max snapshot_id {max_snapshot_id} from i3filter.runs".format(**current_info))

    seasons = config.get_seasons_info()
    current_season = config.getint('DEFAULT', 'Season')

    # First run of current season
    first_run = seasons[current_season]['first']  # 

    # If it is -1 (that means the season hasn't begun yet) replace it with a very high number
    # to enable processing the test runs
    if first_run == -1:
        first_run = 9999999

    # Dfeault last run. If no next season is defined, this value will be kept
    last_run = 9999999

    # If a next season is defined, we need to exclude those test runs
    exclude_next_testruns = [-1]
 
    if current_season + 1 in seasons:
        if seasons[current_season + 1]['first'] > -1:
            # last run of the current season is the start run of the next season minus one
            last_run = seasons[current_season + 1]['first'] - 1
            exclude_next_testruns = seasons[current_season + 1]['test']

    # get the newest data from the live db      
    livesql = """
        SELECT
            r.runNumber, r.tStart, r.tStop,
            r.tStart_frac, r.tStop_frac, r.nEvents, r.rateHz,
            l.snapshot_id, l.good_i3, l.good_it, l.reason_i3, l.reason_it,
            l.good_tstart, l.good_tstart_frac, l.good_tstop,l.good_tstop_frac
        FROM live.livedata_snapshotrun l
        JOIN live.livedata_run r
            ON l.run_id = r.id
        WHERE (r.runNumber >= {first_run} OR r.runNumber IN ({test_runs}))
            AND r.runNumber <= {last_run}
            AND r.runNumber NOT IN ({exclude_next_testruns})
        ORDER BY l.snapshot_id
    """.format(
            first_run = first_run,
            test_runs = ','.join([str(r) for r in seasons[current_season]['test']] + ['-1']),
            last_run = last_run,
            exclude_next_testruns = ','.join(str(r) for r in exclude_next_testruns)
    )
   
    logger.debug("SQL to get data from live: {0}".format(livesql))

    i3live_query = db_i3live.fetchall(livesql)

    if not len(i3live_query):
        logger.info("no results from i3Live DB for runs >= {first_run}".format(first_run = first_run))
        exit(0)

    # dict structure of live db data ensures only latest entry for every run is considered
    run_data = {}

    # Need to catch all run id/snapshot id combinations of all runs
    # That means that it needs to be aware of that one run can have several entries/snapshot ids
    run_snapshot_id_mapping = {}

    for row in i3live_query:
        run_data[row['runNumber']] = row

        # Add run id/snapshot id combination
        run_snapshot_id_mapping[row['runNumber']] = row['snapshot_id']

    run_ids = run_data.keys()
    run_ids.sort()

    run_snapshot_id_str = ",".join(["'{run_id}_{snapshot_id}'".format(run_id = run_id, snapshot_id = snapshot_id) for run_id, snapshot_id in run_snapshot_id_mapping.items()])

    # get all previous runs from filter-db and check if entries in live are different
    runs_in_production_db = db_filter.fetchall("SELECT run_id FROM i3filter.runs ORDER BY run_id")
    new_data = list(set(run_ids) - set([row['run_id'] for row in runs_in_production_db]))
    new_data.sort()
   
    old_data_sql = """
        SELECT run_id
        FROM i3filter.runs
        WHERE CONCAT(run_id, "_", snapshot_id) IN ({run_snapshot_id_str})
        ORDER BY run_id""".format(run_snapshot_id_str = run_snapshot_id_str)

    logger.debug("SQL to get old data from production DB: {0}".format(old_data_sql))

    old_data = [row['run_id'] for row in db_filter.fetchall(old_data_sql)]

    changed_data_sql = """
        SELECT run_id, snapshot_id
        FROM i3filter.runs
        WHERE (run_id >= {first_run} OR run_id IN ({test_runs}))
           AND run_id <= {last_run}
           AND run_id NOT IN ({exclude_next_testruns})
        ORDER BY run_id""".format(
            first_run = first_run,
            test_runs = ','.join([str(r) for r in seasons[current_season]['test']] + ['-1']),
            last_run = last_run,
            exclude_next_testruns = ','.join(str(r) for r in exclude_next_testruns)
    )

    logger.debug("SQL to get data from production DB for changed records: {0}".format(changed_data_sql))

    changed_data = db_filter.fetchall(changed_data_sql)

    # Find out which records have been changed. That means which run has a new snapshot
    # Check which RunId/SnapShotId combinations are already in the DB

    logger.debug("Information from i3live about run/snapshot mapping: {0}".format(run_snapshot_id_mapping))

    new_snapshot_for_run = {}
    for run_id, snapshot_id in run_snapshot_id_mapping.items():
        found = False

        for c in changed_data:
            if run_id == c['run_id'] and snapshot_id == c['snapshot_id']:
                found = True
                break

        logger.debug("run_id = {run_id}, snapshot_id = {snapshot_id}, found = {found}".format(run_id = run_id, snapshot_id = snapshot_id, found = found))

        if not found:
            for c in changed_data:
                if run_id == c['run_id']:
                    new_snapshot_for_run[run_id] = snapshot_id
                    break

    if len(new_snapshot_for_run):
        logger.info("The following records have changed and will result in an update to the production_version {0}".format(new_snapshot_for_run))

        if not check:
            logger.info("Continue processing with updates (Y/N)")
            continue_processing = raw_input("Continue processing with updates (Y/N): ")

            if continue_processing.upper() != "Y":
                logger.info("Halting processig due to user intervention...")
                exit(0)

    changed_runs = new_snapshot_for_run
    if len(changed_runs):
        current_info['production_version'] += 1

    if not len(new_data) and not len(changed_runs):
        logger.info("No records to be inserted/updated. Exiting.")
        exit(0)

    if check:
        logger.info("New records available. This was only a check. Do nothing. Exit.")
        send_check_notification(new_data, changed_runs, logger, dryrun)
        exit(0)

    logger.info('Found: new runs = {new_runs}, updated runs = {updated_runs}'.format(new_runs = len(new_data), updated_runs = len(changed_runs)))

    for r in run_ids:
        current_run_data = run_data[r]

        run = Run(r, logger)
        run.set_data(
            tstart = current_run_data['tStart'],
            tstart_frac = current_run_data['tStart_frac'],
            good_tstart = current_run_data['good_tstart'] or current_run_data['tStart'],
            good_tstart_frac = current_run_data['good_tstart_frac'] or current_run_data['tStart_frac'],
            tstop = current_run_data['tStop'],
            tstop_frac = current_run_data['tStop_frac'],
            good_tstop = current_run_data['good_tstop'] or current_run_data['tStop'],
            good_tstop_frac = current_run_data['good_tstop_frac'] or current_run_data['tStop_frac'],
            good_i3 = current_run_data['good_i3'],
            good_it = current_run_data['good_it'],
            snapshot_id = current_run_data['snapshot_id'],
            production_version = current_info['production_version'],
            nevents = current_run_data['nEvents'],
            rate = current_run_data['rateHz']
        )

        logger.debug("Is run {run_id} a good run? = {is_good_run}".format(run_id = run.run_id, is_good_run = run.is_good_run()))

        check_files = True

        if r in old_data:
            continue

        if r in new_data:
            logger.info("Entering new records for run = {0}".format(r))

            in_files = None

            if inputfiletype == 'PFFilt':
                in_files = run.get_pffilt_files()
            elif inputfiletype == 'PFDST':
                in_files = run.get_pfdst_files()
            else:
                raise Exception('File type `{0}` cannot be handled'.format(inputfiletype))

            detailed_info = {}
            check_files = validate_file_integrity(files = in_files, logger = logger, show_mismatches = run.is_good_run(), detailed_info = detailed_info)

            # Print files that are empty or have wrong permissions
            detailed_info = detailed_info[run.run_id]

            if len(detailed_info['empty_files']) > 0 or len(detailed_info['wrong_permission']) > 0:
                logger.warning("Run {run_id} has issues with input files".format(run_id = run.run_id))

                logger.warning('  Empty files:')
                for f in detailed_info['empty_files']:  
                    logger.warning('    ' + f )

                logger.warning('  Files w/o reading permission:')
                for f in detailed_info['wrong_permission']:
                    logger.warning('    ' + f)

            logger.debug("Check files returned {0}".format(check_files))

        update_comment = ''
        if r in changed_runs.keys():
            logger.info("updating records for run = {0}".format(r))
            update_comment = 'Updated in snapshot {0}'.format(run.get_snapshot_id())

        # Insert new runs from live in filter-db
        if not dryrun and (check_files or not is_good_run):
            run_insertion_sql = """
                INSERT INTO `i3filter`.`runs` (
                    `run_id`, `snapshot_id`, `production_version`, `good_i3`, `good_it`, `reason_i3`, `reason_it`,
                    `good_tstart`, `good_tstart_frac`, `good_tstop`, `good_tstop_frac`, `tstart`, `tstart_frac`, `tstop`, `tstop_frac`,
                    `nevents`, `rate`
                ) VALUES (
                    {run_id}, {snapshot_id}, {production_version}, {good_i3}, {good_it}, '{reason_i3}', '{reason_it}',
                    '{good_tstart}', {good_tstart_frac}, '{good_tstop}', {good_tstop_frac}, '{tstart}', {tstart_frac},
                    '{tstop}', {tstop_frac}, {nevents}, {rate}
                )
            """.format(
                    run_id = run.run_id,
                    snapshot_id = run.get_snapshot_id(),
                    production_version = run.get_production_version(),
                    good_i3 = run.is_good_in_ice_run(),
                    good_it = run.is_good_ice_top_run(),
                    reason_i3 = current_run_data['reason_i3'],
                    reason_it = current_run_data['reason_it'],
                    good_tstart = current_run_data['good_tstart'],
                    good_tstart_frac = current_run_data['good_tstart_frac'],
                    good_tstop = current_run_data['good_tstop'],
                    good_tstop_frac = current_run_data['good_tstop_frac'],
                    tstart = current_run_data['tStart'],
                    tstart_frac = current_run_data['tStart_frac'],
                    tstop = current_run_data['tStop'],
                    tstop_frac = current_run_data['tStop_frac'],
                    nevents = current_run_data['nEvents'],
                    rate = current_run_data['rateHz']
            )

            logger.debug('Run insertion SQL: {0}'.format(run_insertion_sq))
            db_filter.execute(run_insertion_sq)

if __name__ == "__main__":
    parser = get_defaultparser(__doc__, dryrun = True)
    parser.add_argument('--check', help="Only check for updates. Do nothing else", action = "store_true", default = False)
    parser.add_argument('--inputfiletype', help="What is the input file type? Available options: PFDST, PFFilt. Default is PFFilt", default = 'PFFilt', required = False)
    args = parser.parse_args()

    logfile = os.path.join(get_logdir(sublogpath = 'PreProcessing'), 'GetRunInfo_')

    if args.logfile is not None:
        logfile = args.logfile

    logger = get_logger(args.loglevel, logfile)

    if args.inputfiletype not in ['PFDST', 'PFFilt']:
        logger.critical('Input file type must match `PFDST` or `PFFilt`.')
        exit(1)

    main(inputfiletype = args.inputfiletype, logger = logger, dryrun = args.dryrun, check = args.check)


