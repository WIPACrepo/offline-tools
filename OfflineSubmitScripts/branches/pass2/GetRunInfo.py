#!/usr/bin/env python

"""
Gather information about recent runs from live database and migrate it to
the prodcution databases (i3filter on dbs4). Modifies tables grl_snapshot_info_pass2
and run_info_summary_pass2 on dbs4
"""

import os
import re
import sys
from libs.files import get_logdir, get_tmpdir
import libs.config
sys.path.append(libs.config.get_config().get('DEFAULT', 'SQLClientPath'))
import SQLClient_i3live as live
import SQLClient_dbs4 as dbs4
import SQLClient_dbs2 as dbs2

from sys import exit
from libs.logger import get_logger
from libs.argparser import get_defaultparser
import libs.checks
from libs.databaseconnection import DatabaseConnection
from RunTools import RunTools
from icecube.dataclasses import I3Time
from libs.runs import get_validated_runs

import json

dbs4_ = dbs4.MySQL()
m_live = live.MySQL()
dbs2_ = dbs2.MySQL()

def get_gaps_file_data(db, runs):
    sql = "SELECT * FROM sub_runs WHERE run_id IN (%s)" % ','.join([str(r) for r in runs])

    dbdata = db.fetchall(sql, UseDict = True)

    data = {}
    for row in dbdata:
        if row['run_id'] not in data:
            data[row['run_id']] = {}

        data[row['run_id']][row['sub_run']] = row 

    return data

def get_first_last_event(infiles, logger):
    from icecube import dataio

    infiles = sorted(infiles)
    first = infiles[0]
    last = infiles[-1]

    logger.warning('First file: {}'.format(first))
    logger.warning('Last file: {}'.format(last))

    firsti3 = dataio.I3File(first)
    first_header = None
    while first_header is None and firsti3.more():
        frame = firsti3.pop_frame()
        if 'I3EventHeader' in frame:
            first_header = frame['I3EventHeader']

    lasti3 = dataio.I3File(last)
    last_header = None
    while lasti3.more():
        frame = lasti3.pop_frame()
        if 'I3EventHeader' in frame:
            last_header = frame['I3EventHeader']

    return first_header, last_header

def send_check_notification(logger, dryrun, config, new_records, changed_records):
    sys.path.append(config.get('DEFAULT', 'ProductionToolsPath'))
    import SendNotification as SN

    SENDER = config.get('Notifications', 'eMailSender')
    RECEIVERS = libs.config.get_var_list('GetRunInfo', 'NotificationReceiver')
    DOMAIN = config.get('Notifications', 'eMailDomain')
    
    message = ""
    subject = " New snapshot available"
    messageBody = ""
    
    # only necessary for html emails
    mimeVersion="1.0"
    contentType="text/html"
    
    messageBody += """
        *** This is an automated message generated by the *** <br>
        ***        GetRunInfo system of L2 Processing     *** <br><br>
        
        A new snapshot is available!

        New Runs: %s
        Changed Runs: %s
        """ % (new_records, changed_records)
   
    logger.debug("Receivers: %s" % RECEIVERS)
 
    message = SN.CreateMsg(DOMAIN, SENDER, RECEIVERS, subject,messageBody,mimeVersion,contentType)
    
    if len(message) and not dryrun:
        SN.SendMsg(SENDER,RECEIVERS,message)

def read_grl_file(f):
    fh = open(f, 'r')
    first = 2   
    r = {}
    for l in fh: 
        if first > 0:
            first = first - 1 
            continue
        s = l.split()
        r[int(s[0])] = s 
    fh.close()
    return r

def get_lost_file_information(run_id, logger):
    db = DatabaseConnection.get_connection('filter-db', logger)

    lost_files = db.fetchall('SELECT * FROM i3filter.missing_files_pass2 WHERE run_id = {0} AND NOT resolved ORDER BY sub_run'.format(run_id), UseDict = True)

    data = {}

    for f in lost_files:
        data[f['sub_run']] = f

    return data


def main(config, logger,dryrun = False, check = False, updates_only = False, runs = [], ignore_time_mismatch = False, errorfile = None, check_pfdst_dataset = None):
    if check:
        logger.info('Only in check mode. Just checking if an update is available.')

    if updates_only:
        logger.info('Only updates will be executed.')

    validated_runs = None
    if check_pfdst_dataset is not None:
        validated_runs = [row['run_id'] for row in get_validated_runs(check_pfdst_dataset, logger = logger)]

    # Get the current production version and snapshot info 
    # from the production database dbs4
    CurrentInfo = dbs4_.fetchall("""select max(snapshot_id) as maxSnapshotID,
                                    max(production_version) as maxProductionV,
                                    max(ss_ref) as max_ss_ref
                                    from i3filter.grl_snapshot_info_pass2""",UseDict=True)
   
    if CurrentInfo[0]['maxSnapshotID'] is None and CurrentInfo[0]['maxProductionV'] is None and CurrentInfo[0]['max_ss_ref'] is None:
        logger.debug("Very first execution. Anything is Zeroooooooooooo!")
        CurrentInfo = [{'maxSnapshotID': 0, 'maxProductionV': 0, 'max_ss_ref': 0}]
 
    CurrentMaxSnapshot = int(CurrentInfo[0]['maxSnapshotID'])
    CurrentProductionVersion = int(CurrentInfo[0]['maxProductionV'])
    ss_ref = int(CurrentInfo[0]['max_ss_ref']) + 1
    logger.debug("Got current max production_version %i and ss_ref %i from grl_snapshot_info_pass2 table" %(CurrentMaxSnapshot,CurrentProductionVersion)) 
    
    seasons = libs.config.get_seasons_info()
    current_season = config.getint('DEFAULT', 'Season')

    # First run of current season
    IC86_5_FirstRun = seasons[current_season]['first']  # 

    # If it is -1 (that means the season hasn't begun yet) replace it with a very high number
    # to enable processing the test runs
    if IC86_5_FirstRun == -1:
        IC86_5_FirstRun = 9999999

    # Dfeault last run. If no next season is defined, this value will be kept
    IC86_5_LastRun = 9999999

    # If a next season is defined, we need to exclude those test runs
    exclude_next_testruns = [-1]
 
    if current_season + 1 in seasons:
        if seasons[current_season + 1]['first'] > -1:
            # last run of the current season is the start run of the next season minus one
            IC86_5_LastRun = seasons[current_season + 1]['first'] - 1
            exclude_next_testruns = seasons[current_season + 1]['test']

    # get the newest data from the live db      
    livesql = """SELECT r.runNumber,r.tStart,r.tStop,
                    r.tStart_frac,r.tStop_frac,r.nEvents,r.rateHz,
                    l.snapshot_id,l.good_i3,l.good_it,l.reason_i3,l.reason_it,
                    l.good_tstart, l.good_tstart_frac, l.good_tstop,l.good_tstop_frac
                 FROM live.livedata_snapshotrun l
                 JOIN live.livedata_run r
                    ON l.run_id=r.id
                 WHERE (r.runNumber>=%s OR r.runNumber IN (%s))
                    AND r.runNumber<=%s
                    AND r.runNumber NOT IN (%s)
                 ORDER BY l.snapshot_id""" % (IC86_5_FirstRun,
                        ','.join([str(r) for r in seasons[current_season]['test']] + ['-1']),
                        IC86_5_LastRun,
                        ','.join(str(r) for r in exclude_next_testruns))
   
    logger.debug("SQL to get data from live: %s" % livesql)

    tmp_i3_ = m_live.fetchall(livesql, UseDict = True)

    if current_season == 2010:
        logger.warning('The current season is 2010. Since not all runs are in a snapshot, we don\'t have information about if they are good or not. Therefore, we need to check the GRL of IC79 and get the run information from i3live for those runs.')

        good_ic79_runs_data = read_grl_file('/data/exp/IceCube/2010/filtered/level2/IC79_GRLists/IC79_GRL_NewFormat.txt')
        good_ic79_runs = good_ic79_runs_data.keys()

        livesql = """SELECT r.runNumber,r.tStart,r.tStop,
                        r.tStart_frac,r.tStop_frac,r.nEvents,r.rateHz,
                        0 AS snapshot_id, 0 AS good_i3, 0 AS good_it, '' AS reason_i3, '' AS reason_it,
                        NULL AS good_tstart, NULL AS good_tstart_frac, NULL AS good_tstop, NULL AS good_tstop_frac
                     FROM live.livedata_run r
                     WHERE (r.runNumber>=%s OR r.runNumber IN (%s))
                        AND r.runNumber<=%s
                        AND r.runNumber NOT IN (%s)
                     ORDER BY runNumber""" % (IC86_5_FirstRun,
                            ','.join([str(r) for r in seasons[current_season]['test']] + ['-1']),
                            IC86_5_LastRun,
                            ','.join(str(r) for r in exclude_next_testruns))

        all_runs = m_live.fetchall(livesql, UseDict = True)
        all_run_ids = [int(r['runNumber']) for r in all_runs]
        all_already_queried_runs = [int(r['runNumber']) for r in tmp_i3_]

        # Check if we have all good runs
        for r in good_ic79_runs:
            if r not in all_run_ids:
                logger.critical('We do not have information for all good runs.')
                logger.critical('Missing run: {0}'.format(r))
                exit(1)

        # Add run info
        for data in all_runs:
            if data['runNumber'] in good_ic79_runs and data['runNumber'] not in all_already_queried_runs:
                data['good_i3'] = int(good_ic79_runs_data[data['runNumber']][1])
                data['good_it'] = int(good_ic79_runs_data[data['runNumber']][2])
                logger.info('Add IC79 run w/o snapshot: {0}'.format(data['runNumber']))

                logger.debug('data = {0}'.format(data))

                tmp_i3_.append(data)
 
    if not len(tmp_i3_):
        logger.info("no results from i3Live DB for runs>=%s, snapshot_id>%s. no DB info. updated,exiting"%(IC86_5_FirstRun,CurrentMaxSnapshot))
        exit(0)
    
    # dict structure of live db data ensures only latest entry for every run is considered
    RunInfo_ = {}

    # Need to catch all run id/snapshot id combinations of all runs
    # That means that it needs to be aware of that one run can have several entries/snapshot ids
    Run_SSId = {}

    for r_ in tmp_i3_:
        for k in r_.keys():
            if r_[k] is None : r_[k]="NULL"

        RunInfo_[r_['runNumber']] = r_

        # Add run id/snapshot id combination
        Run_SSId[r_['runNumber']] = r_['snapshot_id']
    
    RunNums_ = RunInfo_.keys()
    RunNums_.sort()
    
    Run_SSId_Str_ = ",".join(["'%s_%s'" % (r, Run_SSId[r]) for r in Run_SSId.keys()])
    RunStr_ = ",".join([str(r) for r in RunNums_])

    # get all previous runs from dbs4 and check if entries in live are different
    tmpRecords_ = dbs4_.fetchall("""select DISTINCT run_id from i3filter.grl_snapshot_info_pass2
                                    order by run_id""",UseDict=True)
    tRecords_ = [t_['run_id'] for t_ in tmpRecords_]
    NewRecords_ = list(set(RunNums_).difference(set(tRecords_)))
    NewRecords_.sort()
   
    oRecordsSQL = """SELECT run_id FROM i3filter.grl_snapshot_info_pass2
                             WHERE CONCAT(run_id, "_", snapshot_id) IN (%s)
                             ORDER BY run_id""" % Run_SSId_Str_
    oRecords_ = dbs4_.fetchall(oRecordsSQL, UseDict = True)

    OldRecords_ = [o_['run_id'] for o_ in oRecords_]

    logger.debug("SQL to get old data from production DB: %s" % oRecordsSQL)
    
    cRecordsSQL = """SELECT s.run_id, snapshot_id
                     FROM i3filter.grl_snapshot_info_pass2 r
                     JOIN i3filter.run_info_summary_pass2 s
                        ON s.run_id = r.run_id
                     WHERE (r.run_id >= %s OR r.run_id IN (%s))
                        AND r.run_id <= %s
                        AND r.run_id NOT IN (%s)
                     ORDER BY s.run_id""" % (IC86_5_FirstRun,
                        ','.join([str(r) for r in seasons[current_season]['test']] + ['-1']),
                        IC86_5_LastRun,
                        ','.join(str(r) for r in exclude_next_testruns))
 
    logger.debug("SQL to get data from production DB for changed records: %s" % cRecordsSQL)
 
    cRecords_ = dbs4_.fetchall(cRecordsSQL, UseDict = True)

    # Find out which records have been changed. That means which run has a new snapshot
    # Check which RunId/SnapShotId combinations are already in the DB

    logger.debug("Information from i3live: %s" % Run_SSId)

    newSnapshotForRun = {}
    for run_id, snapshot_id in Run_SSId.iteritems():
        found = False

        for c in cRecords_:
            if run_id == c['run_id'] and snapshot_id == c['snapshot_id']:
                found = True
                break

        logger.debug("run_id = %s, snapshot_id = %s, found = %s" % (run_id, snapshot_id, found))

        if not found:
            for c in cRecords_:
                if run_id == c['run_id']:
                    newSnapshotForRun[run_id] = snapshot_id
                    break

    if len(newSnapshotForRun):
        logger.info("""The following records have changed and will result in an update to the ProductionVersion %s""" % newSnapshotForRun)

        if not check:
            logger.info("Continue processing with updates (Y/N)")
            continueProcessing = raw_input("Continue processing with updates (Y/N) : " )
    
            if continueProcessing.upper() != "Y":
                logger.info("halting processig due to user intervention ...")
                exit (0)
   
    # Hack for season 2010, 2011: Getting start/stop times from pass1 L2 files
    if current_season in (2010, 2011):
        filter_db = DatabaseConnection.get_connection('filter-db', logger)
        gaps_data = get_gaps_file_data(filter_db, RunNums_)
 
    #ChangedRecords_ = [c_['run_id'] for c_ in cRecords_]
    ChangedRecords_ = newSnapshotForRun
    if len(cRecords_):
        CurrentProductionVersion +=1
    
    if not len(NewRecords_) and not len(ChangedRecords_):
        logger.info("no records to be inserted/updated .. exiting")
        exit(0)

    if check:
        logger.info("New records available. This was only a check. Do nothing. Exit.")
        send_check_notification(logger, dryrun, config, NewRecords_, ChangedRecords_)
        exit(0)

    detailed_check_information = {}
    
    for r in RunNums_:
        is_good_run = RunInfo_[r]['good_it'] or RunInfo_[r]['good_i3']
        logger.debug("Is run %s a good run? = %s" % (r, is_good_run))

        if len(runs) and r not in runs:
            logger.debug('Skipping run {} since only specific runs will be reported (see --runs)'.format(r))
            continue

        if validated_runs is not None:
            if r not in validated_runs:
                logger.error('Run {0} has not been validated in dataset {1}. This run will be skipped.'.format(r, check_pfdst_dataset))
                continue

       # if not is_good_run:
       #     logger.info("Skip run %s because it is a bad run" % r)
       #     continue

        # It should not matter if they are only good_it
        #if r in [119194, 119398]:
        #    logger.warning("Skip run %s because it has bad InIce % r")
        #    continue

        logger.debug('Current run: %s' % r)

        if r in OldRecords_ : continue
        if r in NewRecords_:
            if updates_only:
                logger.info('%s is a new run but the `updates-only` option is enabled. Skip this run.' % r)
                continue

            gb = ('BAD', 'GOOD')
            logger.info("entering new records for run = %s (%s)" % (r, gb[is_good_run]))

            R = RunTools(r, logger = logger, passNumber = 2)
            RunTimes = R.GetRunTimes()
            InFiles = R.GetRunFiles(RunTimes['tStart'],'P', season = current_season)

            # Exclude log files etc
            InFiles = [f for f in InFiles if '.log' not in f]
            for e in config.get('DEFAULT', 'IgnoreFilesWithExtension').split(','):
                InFiles = [f for f in InFiles if not f.endswith(e)]

#            logger.info('infiles = {}'.format(InFiles))

            CheckFiles = R.FilesComplete(InFiles, RunTimes, get_tmpdir(), showTimeMismatches = is_good_run, outdict = detailed_check_information, no_xml_bundle = current_season in (2010, 2015, 2016))

            logger.debug("Check files returned %s" % CheckFiles)
            logger.debug('detailed_check_information = {}'.format(detailed_check_information))

            lost_files = get_lost_file_information(r, logger)
            if len(lost_files):
                logger.warning('For this run {} files has been marked as lost. We will ignore warnings for those lost files. Enable --debug if you would like to see the detailed information.'.format(len(lost_files)))

                logger.debug('The following files are lost:')
                for sub_run in sorted(lost_files):
                    logger.debug(lost_files[sub_run])

                # If a missing file parted matches, remove it from the "missing file list":
                for sub_run in detailed_check_information[r]['missing_files']:
                    if sub_run in lost_files:
                        logger.info('File #{} has been marked as lost. We will ignore this missing file.'.format(sub_run))
                    else:
                        logger.error('File #{} has NOT been marked as lost! This is an error!'.format(sub_run))

                detailed_check_information[r]['missing_files'] = [s for s in detailed_check_information[r]['missing_files'] if s not in lost_files]

                logger.info('Finally, there are {} files missing!'.format(len(detailed_check_information[r]['missing_files'])))

                if not len(detailed_check_information[r]['missing_files']) and \
                  not detailed_check_information[r]['metadata_start_time_error'] and \
                  not detailed_check_information[r]['metadata_stop_time_error'] and \
                  len(detailed_check_information[r]) == 6 and not len(detailed_check_information[r]['duplicates']):
                    CheckFiles = 1

                if not CheckFiles and not len(detailed_check_information[r]['missing_files']) and \
                  (detailed_check_information[r]['metadata_start_time_error'] or detailed_check_information[r]['metadata_stop_time_error']) and \
                  len(detailed_check_information[r]) == 5 and not len(detailed_check_information[r]['duplicates']):
                    logger.warning('We have time mismatche(s) AND we have lost files. It could be that the mismatche(s) are a result of the missing files.')
                    logger.warning('Ignore the time mismatches since it will be checked again at the post processing step where the missing files are taken into account.')
                    CheckFiles = 1

            logger.debug("Check files returned %s" % CheckFiles)
            logger.debug('detailed_check_information = {}'.format(detailed_check_information))

            # Hack for season 2012: We agreed that we're using the pDAQ/tstart/tstop times (meeting 02/01/2017, John, Dave, Matt, Michael, Colin, Jan)
            # Hack for season 2011: We agreed that we don't have any `good` time information at all. Therefore, we will process the entire files.
            if current_season in [2010, 2011, 2012] and not CheckFiles:
                # Check why the CheckFiles went wrong
                # If it is only the tstart/tstop time, we'll ignore it
                if len(detailed_check_information[r]['missing_files']) == 0 and not len(detailed_check_information[r]['duplicates']):
                    # OK, the only error is a mismatch in start or stop time. We will igore that:
                    CheckFiles = 1

                if current_season == 2012:
                    logger.warning("*** You are currently import runs of season 2012. We agreed to ignore time mismatches and use the pDAQ times. ***")
                elif current_season in (2010, 2011):
                    logger.warning("*** You are currently import runs of season 2011. We agreed to ignore time mismatches and process the entire file. ***")
  
            if ignore_time_mismatch and not CheckFiles:
                if len(detailed_check_information[r]['missing_files']) == 0\
                  and (detailed_check_information[r]['metadata_start_time_error'] or detailed_check_information[r]['metadata_stop_time_error']) \
                  and not len(detailed_check_information[r]['duplicates']):
                    # OK, the only error is a mismatch in start or stop time. We will igore that:
                    CheckFiles = 1
                    logger.warning('You enabled the option --ignore-time-mismatch. The time mis match will be ignored.')

            logger.debug("Check files returned %s" % CheckFiles)

            #  fill new runs from live in run_info_summary_pass2 
            if not dryrun and (CheckFiles or not is_good_run):
                logger.debug("Insert run into run_info_summary_pass2")

                dbs4_.execute("""INSERT IGNORE INTO i3filter.run_info_summary_pass2
                                 (run_id, tStart, tStop, tStart_frac, tStop_frac, nEvents, rateHz, FilesComplete)
                                 VALUES (%u, "%s", "%s", "%s", "%s", %s, %s, %u) """ \
                                 % (r, RunInfo_[r]['tStart'], RunInfo_[r]['tStop'],
                                 RunInfo_[r]['tStart_frac'], RunInfo_[r]['tStop_frac'],
                                 RunInfo_[r]['nEvents'], RunInfo_[r]['rateHz'], CheckFiles))
        
        reason_i3 = ""
        reason_i3 = re.sub(r'[",]','',",".join(RunInfo_[r]['reason_i3'][1:-1].split(",")))
        reason_it = "" 
        reason_it = re.sub(r'[",]','',",".join(RunInfo_[r]['reason_it'][1:-1].split(",")))
    
        UpdateComment = ''    
        if r in ChangedRecords_.keys():
            logger.info("updating records for run = %s"%r)
            UpdateComment = 'Updated in snapshot'

            # Just updating the Snapshot. Files were already checked the first time
            CheckFiles = 1
    
        goodStart = RunInfo_[r]['tStart']
        if RunInfo_[r]['good_tstart'] != "NULL" : goodStart = RunInfo_[r]['good_tstart']
        goodStart_frac = RunInfo_[r]['tStart_frac']
        if RunInfo_[r]['good_tstart_frac'] != "NULL" : goodStart_frac = RunInfo_[r]['good_tstart_frac']
        
        goodStop = RunInfo_[r]['tStop']
        if RunInfo_[r]['good_tstop'] != "NULL"  : goodStop = RunInfo_[r]['good_tstop']
        goodStop_frac = RunInfo_[r]['tStop_frac']
        if RunInfo_[r]['good_tstop_frac'] != "NULL"  : goodStop_frac = RunInfo_[r]['good_tstop_frac']

        # Hack for season 2011: Use as good start/stop times the PFDS start/end time
        if current_season in (2010, 2011) and is_good_run:
            if r not in gaps_data:
                # If the run does not appear in pass1, we need to find the start/stop info in the files
                if  detailed_check_information[r]['metadata_start_time'] is None or  detailed_check_information[r]['metadata_stop_time'] is None:
                    logger.fatal('*** Run %s of season %s does not have proper start/stop meta information. ***' % (r, current_season))
                    exit(-1)
                else:
                    logger.warning('No pass1 metadata available. going through the first and last file to find the first and last event header...')
                    first_header, last_header = get_first_last_event(InFiles, logger)

                    gaps_data_first_sr = first_header.start_time
                    gaps_data_last_sr = last_header.end_time
            else:
                first_sr = min(gaps_data[r].keys())
                last_sr = max(gaps_data[r].keys())

                gaps_data_first_sr = I3Time(gaps_data[r][first_sr]['first_event_year'], gaps_data[r][first_sr]['first_event_frac'])
                gaps_data_last_sr = I3Time(gaps_data[r][last_sr]['last_event_year'], gaps_data[r][last_sr]['last_event_frac'])

            logger.warning('*** Old start/stop time information: good_tstart = %s, good_tstart_frac = %s, good_tstop = %s, good_tstop_frac = %s ***' % (goodStart, goodStart_frac, goodStop, goodStop_frac))

            goodStart = gaps_data_first_sr.date_time
            goodStart_frac = gaps_data_first_sr.utc_nano_sec * 10
            goodStop = gaps_data_last_sr.date_time
            goodStop_frac = gaps_data_last_sr.utc_nano_sec * 10

            logger.warning('*** New start/stop time information: good_tstart = %s, good_tstart_frac = %s, good_tstop = %s, good_tstop_frac = %s ***' % (goodStart, goodStart_frac, goodStop, goodStop_frac))

        # Check PFFilt files if there are empty and/or have no reading permission
        fileChkRlt = libs.checks.pffilt_size_and_permission(r, RunInfo_[r]['tStart'].year, RunInfo_[r]['tStart'].month, RunInfo_[r]['tStart'].day, logger, False)
        if len(fileChkRlt['empty']) + len(fileChkRlt['permission']) + len(fileChkRlt['emptyAndPermission']) > 0:
            logger.warning("Run %s has issues with PFFilt files" % r)

            logger.warning('  Empty files w/o reading permission:')
            for file in fileChkRlt['emptyAndPermission']:
                logger.warning('    ' + file)
    
            logger.warning('  Empty files w/ reading permission:')
            for file in fileChkRlt['empty']:  
                logger.warning('    ' + file )
    
            logger.warning('  Not empty files w/o reading permission:')
            for file in fileChkRlt['permission']:
                logger.warning('    ' + file)
    
        logger.debug('Insert run %s in DB' % r)

        # insert new runs from live in grl_snapshot_info_pass2
        if not dryrun and (CheckFiles or not is_good_run): dbs4_.execute( """insert into i3filter.grl_snapshot_info_pass2
                            (ss_ref,run_id,snapshot_id,good_i3,good_it,reason_i3,reason_it,
                            production_version,submitted,comments,good_tstart,good_tstart_frac,
                            good_tstop,good_tstop_frac)
                            values(%u,%u,%u,%u,%u,"%s","%s",%u,%u,"%s","%s",%s,"%s",%s)
                            """%(ss_ref,r,RunInfo_[r]['snapshot_id'],RunInfo_[r]['good_i3'],RunInfo_[r]['good_it'],
                            reason_i3,reason_it,CurrentProductionVersion,0,UpdateComment,
                            goodStart,goodStart_frac,goodStop,goodStop_frac))
        elif not dryrun or not (CheckFiles or not is_good_run):
            logger.error('Run as not been inserted into DB. Check other errors and warnings.')
        
        ss_ref+=1

    if errorfile is not None:
        logger.info('Write errors into JSON file: {}'.format(errorfile))
        with open(errorfile, 'w') as f:
            json.dump(detailed_check_information, f, default = lambda o: str(o), sort_keys = True, indent = 2)

if __name__ == "__main__":
    config = libs.config.get_config()

    parser = get_defaultparser(__doc__,dryrun=True)

    parser.add_argument("--runs", type = int, nargs = '*', default = [], required = False, help = "Importing only specific runs")
    parser.add_argument('--check', help="Only check for updates. Do nothing else",dest="check",action="store_true",default=False)  
    parser.add_argument('--ignore-time-mismatch', help="ONLY USE THIS OPTION IF YOU KNOW WHAT YOU ARE DOING!11!111. Sometimes the pass2 times (PFDST) do not match the PFFilt times of pass1. We want to match the pass1 times. Use this option, if the pass2 data is OUTSIDE!!!! of the pass1 range. If it is inside, check for missing data/files.",action="store_true",default=False)  
    parser.add_argument('--updates-only', help="Do only execute updates. Do not insert new entries.",dest="updates_only",action="store_true",default=False)  
    parser.add_argument("--errorfile", type = str, default = None, required = False, help = "Write errors into a json file")
    parser.add_argument("--check-pfdst-dataset", type = int, default = None, required = False, help = "Check dataset if runs have been validated.")

    args = parser.parse_args()
    LOGFILE=os.path.join(get_logdir(sublogpath = 'PreProcessing'), 'GetRunInfo_')
    logger = get_logger(args.loglevel,LOGFILE)

    if args.ignore_time_mismatch and len(args.runs) != 1:
        logger.critical('You can only use --ignore-time-mismatch if you specify exactly one run with --runs! That\'s a safety restriction in orde rto prevent a batch of wrong data.')
        exit(1)

    main(logger = logger, dryrun=args.dryrun, check = args.check, config = config, updates_only = args.updates_only, runs = args.runs, ignore_time_mismatch = args.ignore_time_mismatch, errorfile = args.errorfile, check_pfdst_dataset = args.check_pfdst_dataset)


